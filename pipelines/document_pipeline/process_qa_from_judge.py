#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from collections import defaultdict
import math
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

from tqdm import tqdm

from boundingdoc.mm_agent import MMAgent
from boundingdoc.qa import QAGenerationResult, QAGeneratorConfig, QwenQAGenerator


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate QA pairs for crops listed in judge_output JSONL files."
    )
    parser.add_argument(
        "--judge_output_dir",
        required=True,
        type=Path,
        help="Directory containing merged judge output JSONL files (one per document).",
    )
    parser.add_argument(
        "--work_root",
        required=True,
        type=Path,
        help="Work directory that contains processed/<doc>/<page>/clean_crops images.",
    )
    parser.add_argument(
        "--output_dir",
        required=True,
        type=Path,
        help="Directory where QA JSONL files will be written.",
    )
    parser.add_argument(
        "--qa_model",
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="Model name or path used for QA generation.",
    )
    parser.add_argument(
        "--qa_max_new_tokens",
        type=int,
        default=512,
        help="Maximum tokens generated by the QA model (default: 512).",
    )
    parser.add_argument(
        "--qa_temperature",
        type=float,
        default=0.0,
        help="Sampling temperature for QA generation (default: 0.0).",
    )
    parser.add_argument(
        "--qa_top_p",
        type=float,
        default=1.0,
        help="Top-p nucleus sampling value for QA generation (default: 1.0).",
    )
    parser.add_argument(
        "--qa_repetition_penalty",
        type=float,
        default=1.05,
        help="Repetition penalty applied during QA generation (default: 1.05).",
    )
    parser.add_argument(
        "--qa_backend",
        choices=("hf", "vllm"),
        default="hf",
        help="Backend for QA generation inference (default: hf).",
    )
    parser.add_argument(
        "--qa_gpu_devices",
        type=str,
        help="Comma-separated GPU ids used by the QA backend (relevant for vLLM or single-GPU HF agent).",
    )
    parser.add_argument(
        "--qa_batch_size",
        type=int,
        default=1,
        help="Number of crops per QA request when using the agent backend (default: 1).",
    )
    parser.add_argument(
        "--qa_workers",
        type=int,
        default=1,
        help="Number of parallel QA worker threads (used by vLLM backend).",
    )
    parser.add_argument(
        "--raw_output_dir",
        type=Path,
        help="Optional directory where raw model outputs will be stored. Defaults to <output_dir>/qa_raw_outputs.",
    )
    return parser.parse_args()


def _parse_devices(arg: Optional[str]) -> Optional[List[int]]:
    if not arg:
        return None
    devices: List[int] = []
    for part in arg.split(","):
        part = part.strip()
        if not part:
            continue
        try:
            devices.append(int(part))
        except ValueError:
            continue
    return devices or None


def _chunk(seq: Sequence, size: int) -> Iterable[Sequence]:
    size = max(1, size)
    for idx in range(0, len(seq), size):
        yield seq[idx : idx + size]


def _load_judge_records(path: Path) -> List[dict]:
    records: List[dict] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                records.append(json.loads(line))
            except Exception:
                continue
    return records


def _resolve_image_path(work_root: Path, doc: str, page: str, image: str) -> Path:
    return work_root / doc / "processed" / doc / page / "clean_crops" / image


def ensure_generator(args: argparse.Namespace) -> QwenQAGenerator:
    config = QAGeneratorConfig(
        model_name=args.qa_model,
        max_new_tokens=args.qa_max_new_tokens,
        temperature=args.qa_temperature,
        top_p=args.qa_top_p,
        repetition_penalty=args.qa_repetition_penalty,
        batch_size=args.qa_batch_size,
        workers=max(1, args.qa_workers),
    )
    agent: MMAgent | None = None
    if args.qa_backend == "vllm":
        agent = MMAgent(
            model_name=args.qa_model,
            use_vllm=True,
            gpu_devices=_parse_devices(args.qa_gpu_devices),
            max_new_tokens=args.qa_max_new_tokens,
        )
    elif args.qa_backend == "hf" and args.qa_gpu_devices:
        agent = MMAgent(
            model_name=args.qa_model,
            use_vllm=False,
            gpu_devices=_parse_devices(args.qa_gpu_devices),
            max_new_tokens=args.qa_max_new_tokens,
        )
    return QwenQAGenerator(config, agent_backend=agent)


def generate_doc_qas(
    doc: str,
    jobs: List[dict],
    generator: QwenQAGenerator,
    output_dir: Path,
    raw_output_dir: Path | None,
) -> Tuple[Path, int]:
    doc_output = output_dir / f"{doc}_qa.jsonl"
    doc_output.parent.mkdir(parents=True, exist_ok=True)
    if raw_output_dir is not None:
        raw_output_dir.mkdir(parents=True, exist_ok=True)

    effective_batch = max(1, generator.config.batch_size)
    if not generator.supports_batch:
        effective_batch = 1

    total_pairs = 0
    total_chunks = math.ceil(len(jobs) / effective_batch)
    with doc_output.open("w", encoding="utf-8") as f:
        for chunk in tqdm(_chunk(jobs, effective_batch), total=total_chunks, desc=f"QA {doc}", unit="img", leave=False):
            image_paths = [job["image_path"] for job in chunk]
            qa_results: List[QAGenerationResult]
            try:
                qa_results = list(generator.generate_batch(image_paths))
            except Exception:
                qa_results = []
                for job in chunk:
                    try:
                        qa_results.append(generator.generate_for_image(job["image_path"]))
                    except Exception:
                        qa_results.append(QAGenerationResult([], ""))

            if len(qa_results) != len(chunk):
                qa_results = qa_results[: len(chunk)]
                while len(qa_results) < len(chunk):
                    qa_results.append(QAGenerationResult([], ""))

            for job, qa_result in zip(chunk, qa_results):
                if raw_output_dir is not None:
                    raw_path = raw_output_dir / f"{job['page']}_{job['image']}.txt"
                    try:
                        raw_path.write_text(qa_result.raw_output, encoding="utf-8")
                    except Exception:
                        pass

                record = {
                    "doc": doc,
                    "page": job["page"],
                    "image": job["image"],
                    "type": job.get("type", "unknown"),
                    "bbox": job.get("bbox"),
                    "qas": [qa.to_dict() for qa in qa_result.qas],
                }
                total_pairs += len(record["qas"])
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
    return doc_output, total_pairs


def main() -> None:
    args = parse_args()
    generator = ensure_generator(args)

    judge_output_dir = args.judge_output_dir.resolve()
    work_root = args.work_root.resolve()
    output_dir = args.output_dir.resolve()
    raw_output_dir = (
        args.raw_output_dir.resolve()
        if args.raw_output_dir
        else output_dir / "qa_raw_outputs"
    )

    jsonl_files = sorted(judge_output_dir.glob("*.jsonl"))
    if not jsonl_files:
        print(f"ERROR: no JSONL files found in {judge_output_dir}")
        return

    jobs_by_doc: Dict[str, List[dict]] = defaultdict(list)
    for jsonl_path in jsonl_files:
        records = _load_judge_records(jsonl_path)
        for record in records:
            doc = record.get("doc")
            page = record.get("page")
            image = record.get("image")
            if not doc or not page or not image:
                continue
            image_path = _resolve_image_path(work_root, doc, page, image)
            if not image_path.is_file():
                print(f"WARNING: missing image file {image_path}, skipping.")
                continue
            job = {
                "doc": doc,
                "page": page,
                "image": image,
                "type": record.get("type", "unknown"),
                "bbox": record.get("bbox"),
                "image_path": image_path,
            }
            jobs_by_doc[doc].append(job)

    if not jobs_by_doc:
        print("ERROR: no valid jobs found after verifying image paths.")
        return

    overall_pairs = 0
    overall_images = 0
    processed_docs = 0

    for doc, jobs in sorted(jobs_by_doc.items()):
        overall_images += len(jobs)
        doc_raw_dir = raw_output_dir / doc if raw_output_dir else None
        output_path, doc_pairs = generate_doc_qas(doc, jobs, generator, output_dir, doc_raw_dir)
        processed_docs += 1
        overall_pairs += doc_pairs
        print(
            f"SUCCESS: {doc} â†’ {output_path} (images={len(jobs)}, qa_pairs={doc_pairs})"
        )

    print(
        f"\nSummary: processed {processed_docs} document(s); "
        f"generated QA for {overall_images} image(s); total QA pairs={overall_pairs}."
    )


if __name__ == "__main__":
    main()
