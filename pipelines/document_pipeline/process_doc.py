#!/usr/bin/env python3
from __future__ import annotations

import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock
from typing import Iterable, List, Optional

from boundingdoc.mm_agent import MMAgent
from boundingdoc.pdf import PdfToPngConfig
from boundingdoc.pipeline import (
    DocumentPipelineConfig,
    DocumentProcessor,
    DocumentProcessingError,
    PipelineResources,
)
from boundingdoc.sam_crop import SamCropConfig
from boundingdoc.judge import JudgeConfig
from boundingdoc.qa import QAGeneratorConfig


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run the full BoundingDoc pipeline for one or more PDFs and produce QA jsonl outputs.",
    )
    parser.add_argument("pdf", type=Path, help="PDF file or directory containing PDF files to process")
    parser.add_argument(
        "--work_root",
        required=True,
        type=Path,
        help="Working directory used to store intermediate artefacts (pages, crops, etc.)",
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        help="Optional directory where the final {doc}.jsonl files will be copied",
    )
    parser.add_argument("--dpi", type=int, default=300, help="DPI used when rasterising PDF pages (default: 300)")
    parser.add_argument(
        "--pdf_thread_count",
        type=int,
        default=None,
        help="Per-PDF rendering threads passed to pdf2image/pdftoppm",
    )
    parser.add_argument(
        "--pdf_max_workers",
        type=int,
        default=None,
        help="Number of PDFs to convert concurrently",
    )
    parser.add_argument(
        "--sam_checkpoint",
        required=True,
        type=str,
        help="Path to the SAM checkpoint file",
    )
    parser.add_argument("--sam_device", default="cuda", help="Device for SAM inference (default: cuda)")
    parser.add_argument(
        "--sam_devices",
        nargs="+",
        default=None,
        help="Optional list of CUDA device identifiers dedicated to SAM (e.g. 4 5 6 7)",
    )
    parser.add_argument(
        "--sam_num_workers",
        type=int,
        default=None,
        help="Number of parallel SAM worker processes (defaults to number of devices)",
    )
    parser.add_argument(
        "--sam_queue_size",
        type=int,
        default=None,
        help="Maximum queued SAM images per worker (defaults to 32)",
    )
    parser.add_argument("--sam_pad_px", type=int, default=10, help="Padding applied around crops in pixels")
    parser.add_argument(
        "--sam_min_ratio",
        type=float,
        default=0.05,
        help="Minimum crop area ratio relative to the page (default: 0.05)",
    )
    parser.add_argument(
        "--sam_max_ratio",
        type=float,
        default=0.70,
        help="Maximum crop area ratio relative to the page (default: 0.70)",
    )
    parser.add_argument(
        "--judge_model",
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="Model name or path used for crop judging",
    )
    parser.add_argument(
        "--judge_max_new_tokens",
        type=int,
        default=128,
        help="Maximum tokens generated by the judge model (default: 128)",
    )
    parser.add_argument(
        "--qa_model",
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="Model name or path used for QA generation",
    )
    parser.add_argument(
        "--qa_max_new_tokens",
        type=int,
        default=512,
        help="Maximum tokens generated by the QA model (default: 512)",
    )
    parser.add_argument(
        "--qa_temperature",
        type=float,
        default=0,
        help="Sampling temperature for QA generation (default: 0)",
    )
    parser.add_argument(
        "--qa_top_p",
        type=float,
        default=1,
        help="Top-p nucleus sampling value for QA generation (default: 1)",
    )
    parser.add_argument(
        "--qa_repetition_penalty",
        type=float,
        default=1.05,
        help="Repetition penalty applied during QA generation (default: 1.05)",
    )
    parser.add_argument(
        "--judge_backend",
        choices=("hf", "vllm"),
        default="hf",
        help="Backend for judge model inference (default: hf)",
    )
    parser.add_argument(
        "--judge_gpu_devices",
        type=str,
        help="Comma-separated GPU ids used by the judge backend (only relevant for vLLM or single-GPU HF)",
    )
    parser.add_argument(
        "--qa_backend",
        choices=("hf", "vllm"),
        default="hf",
        help="Backend for QA generation inference (default: hf)",
    )
    parser.add_argument(
        "--qa_gpu_devices",
        type=str,
        help="Comma-separated GPU ids used by the QA backend (only relevant for vLLM or single-GPU HF)",
    )
    parser.add_argument(
        "--vlm_min_pixels",
        type=int,
        default=256 * 28 * 28,
        help="Minimum pixel count used by the VLM processor (default: 200704)",
    )
    parser.add_argument(
        "--vlm_max_pixels",
        type=int,
        default=1280 * 28 * 28,
        help="Maximum pixel count used by the VLM processor (default: 1003520)",
    )
    parser.add_argument(
        "--share_vlm_agent",
        action="store_true",
        help="Reuse a single VLM agent instance for both judge and QA when models match",
    )
    parser.add_argument(
        "--keep_workdir",
        action="store_true",
        help="Do not overwrite existing per-document working directories",
    )
    parser.add_argument(
        "--judge_batch_size",
        type=int,
        default=1,
        help="Number of crops to judge per vLLM request (default: 1)",
    )
    parser.add_argument(
        "--qa_batch_size",
        type=int,
        default=1,
        help="Number of crops to generate QA for per vLLM request (default: 1)",
    )
    parser.add_argument(
        "--qa_workers",
        type=int,
        default=1,
        help="Number of parallel QA worker threads (vLLM backend only).",
    )
    parser.add_argument(
        "--max_workers",
        type=int,
        default=1,
        help="Number of documents to process concurrently (default: 1)",
    )
    return parser.parse_args()


def _iter_pdfs(path: Path) -> Iterable[Path]:
    if path.is_file() and path.suffix.lower() == ".pdf":
        yield path.resolve()
        return
    if not path.is_dir():
        return
    for pdf in sorted(path.glob("*.pdf")):
        if pdf.is_file():
            yield pdf.resolve()


def _parse_devices(arg: Optional[str]) -> Optional[List[int]]:
    if not arg:
        return None
    devices: List[int] = []
    for part in arg.split(","):
        part = part.strip()
        if not part:
            continue
        devices.append(int(part))
    return devices or None


def main() -> None:
    args = parse_args()
    pdfs: List[Path] = list(_iter_pdfs(args.pdf))
    if not pdfs:
        print(f"ERROR: no PDF files found at {args.pdf}")
        return

    judge_agent: Optional[MMAgent] = None
    qa_agent: Optional[MMAgent] = None

    if args.judge_backend == "vllm":
        judge_agent = MMAgent(
            model_name=args.judge_model,
            use_vllm=True,
            gpu_devices=_parse_devices(args.judge_gpu_devices),
            max_new_tokens=args.judge_max_new_tokens,
            min_pixels=args.vlm_min_pixels,
            max_pixels=args.vlm_max_pixels,
        )
    elif args.judge_backend == "hf" and args.share_vlm_agent:
        judge_agent = MMAgent(
            model_name=args.judge_model,
            use_vllm=False,
            gpu_devices=_parse_devices(args.judge_gpu_devices),
            max_new_tokens=args.judge_max_new_tokens,
            min_pixels=args.vlm_min_pixels,
            max_pixels=args.vlm_max_pixels,
        )

    if args.qa_backend == "vllm":
        if args.share_vlm_agent and judge_agent and args.qa_model == args.judge_model:
            qa_agent = judge_agent
            qa_agent.max_new_tokens = args.qa_max_new_tokens
        else:
            qa_agent = MMAgent(
                model_name=args.qa_model,
                use_vllm=True,
                gpu_devices=_parse_devices(args.qa_gpu_devices),
                max_new_tokens=args.qa_max_new_tokens,
                min_pixels=args.vlm_min_pixels,
                max_pixels=args.vlm_max_pixels,
            )
    elif args.qa_backend == "hf" and args.share_vlm_agent:
        if judge_agent and args.qa_model == args.judge_model:
            qa_agent = judge_agent
            qa_agent.max_new_tokens = args.qa_max_new_tokens
        else:
            qa_agent = MMAgent(
                model_name=args.qa_model,
                use_vllm=False,
                gpu_devices=_parse_devices(args.qa_gpu_devices),
                max_new_tokens=args.qa_max_new_tokens,
                min_pixels=args.vlm_min_pixels,
                max_pixels=args.vlm_max_pixels,
            )

    pipeline_config = DocumentPipelineConfig(
        work_root=args.work_root,
        sam=SamCropConfig(
            checkpoint=args.sam_checkpoint,
            device=args.sam_device,
            pad_px=args.sam_pad_px,
            area_min_ratio=args.sam_min_ratio,
            area_max_ratio=args.sam_max_ratio,
            devices=tuple(
                f"cuda:{dev}" if str(dev).isdigit() else str(dev)
                for dev in args.sam_devices
            )
            if args.sam_devices
            else None,
            num_workers=args.sam_num_workers,
            queue_size=args.sam_queue_size or 32,
        ),
        judge=JudgeConfig(
            model_name=args.judge_model,
            max_new_tokens=args.judge_max_new_tokens,
            batch_size=args.judge_batch_size,
        ),
        qa=QAGeneratorConfig(
            model_name=args.qa_model,
            max_new_tokens=args.qa_max_new_tokens,
            temperature=args.qa_temperature,
            top_p=args.qa_top_p,
            repetition_penalty=args.qa_repetition_penalty,
            batch_size=args.qa_batch_size,
            workers=max(1, args.qa_workers),
        ),
        pdf=PdfToPngConfig(
            dpi=args.dpi,
            thread_count=args.pdf_thread_count,
            max_workers=args.pdf_max_workers,
        ),
        overwrite=not args.keep_workdir,
        judge_agent=judge_agent,
        qa_agent=qa_agent,
    )

    max_workers = max(1, args.max_workers)
    max_workers = min(max_workers, len(pdfs))

    resources = PipelineResources(
        sam_lock=Lock(),
        judge_lock=Lock() if pipeline_config.judge_agent is None else None,
        qa_lock=Lock() if pipeline_config.qa_agent is None else None,
    )
    processor = DocumentProcessor(pipeline_config, resources=resources)

    successes: List[str] = []
    failures: List[str] = []

    def _process(pdf_path: Path) -> None:
        try:
            result = processor.process(pdf_path, args.output_dir)
            print(
                f"SUCCESS: {pdf_path.name} → {result.qa_output} "
                f"(pages={result.pages}, crops={result.total_crops}, "
                f"clean={result.clean_crops}, qa_images={result.qa_images}, "
                f"qa_pairs={result.qa_pairs})"
            )
            successes.append(str(result.qa_output))
        except DocumentProcessingError as exc:
            print(f"ERROR: {pdf_path.name} → {exc}")
            failures.append(f"{pdf_path}: {exc}")
        except Exception as exc:  # pragma: no cover
            print(f"ERROR: {pdf_path.name} → {exc}")
            failures.append(f"{pdf_path}: {exc}")

    if max_workers == 1:
        for pdf_path in pdfs:
            _process(pdf_path)
    else:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_process, pdf_path): pdf_path for pdf_path in pdfs}
            for future in as_completed(futures):
                # Ensure exceptions are raised inside _process to be recorded uniformly.
                future.result()

    print(f"\nCompleted {len(pdfs)} document(s).")
    if successes:
        print(f"  ✔ Successful: {len(successes)}")
    if failures:
        print(f"  ✖ Failed: {len(failures)}")
        for item in failures:
            print(f"    - {item}")


if __name__ == "__main__":
    main()
